{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog Assignment\n",
    "\n",
    "For our project we want to merge the following three datasets:\n",
    "\n",
    "- Netflix Originals Movies with IMDB scores \n",
    "\n",
    "- General Netflix Series / Movies data  (From this we want to add the rating column to our Dataframe)\n",
    "\n",
    "- Netflix Stockprices \n",
    "\n",
    "With the merged Dataframe we'll do extensive data analysis with help from Pandas for data manipulation and Plotly to make interactive visualisations.\n",
    "Our goal is to discover if there is any corralation between the different features in this dataset. Examples of this can be corralation with movie releases and stock prices, IMDB scores and stock prices, rating and IMDB score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Kaggle API to download the datasets we want to use during our project\n",
    "\n",
    "The first step is to install the Kaggle library, and use it download the datasets.\n",
    "\n",
    "- Documentation link: https://www.kaggle.com/docs/api\n",
    "- Code source: https://www.youtube.com/watch?v=DgGFhQmfxHo&t=331s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from kaggle) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from kaggle) (4.64.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from kaggle) (1.26.9)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gebruiker\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gebruiker\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->kaggle) (0.4.4)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73049 sha256=bf889e8369bf17bdee997940f61af38d6b1fc79105646bd16316d41e5db86c9e\n",
      "  Stored in directory: c:\\users\\gebruiker\\appdata\\local\\pip\\cache\\wheels\\ac\\b2\\c3\\fa4706d469b5879105991d1c8be9a3c2ef329ba9fe2ce5085e\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.12\n"
     ]
    }
   ],
   "source": [
    "# install the Kaggle library \n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle requires that the authentication key is present on our device ( using the following path: ~/.kaggle/kaggle.json), so we'll have to make a directory before we can proceed any futher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Try to import the Kaggle library. Before proceeding create the directory specified below, and place the kaggle.json there\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import the Kaggle library again\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Kaggle API from the Kaggle library\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully connected to the Kaggle API!\n"
     ]
    }
   ],
   "source": [
    "# instantiate the API, then authenticate (uses the kaggle.json for login credentials)\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "print(\"Succesfully connected to the Kaggle API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Content-Length': '0', 'Date': 'Tue, 27 Sep 2022 07:57:38 GMT', 'Access-Control-Allow-Credentials': 'true', 'Set-Cookie': 'ka_sessionid=398f32e211b231df23a997d72eb1035e; max-age=2626560; path=/, GCLB=CJi2rd6S_oi5kwE; path=/; HttpOnly', 'Turbolinks-Location': 'https://www.kaggle.com/api/v1/datasets/download/luiscorter/netflix-original-films-imdb-scores/NetflixOriginals.csv', 'Strict-Transport-Security': 'max-age=63072000; includeSubDomains; preload', 'Content-Security-Policy': \"object-src 'none'; script-src 'nonce-kqx/Ru4KIml5TH9I82XBVA==' 'report-sample' 'unsafe-inline' 'unsafe-eval' 'strict-dynamic' https: http:; frame-src 'self' https://www.kaggleusercontent.com https://www.youtube.com/embed/ https://polygraph-cool.github.io https://www.google.com/recaptcha/ https://form.jotform.com https://submit.jotform.us https://submit.jotformpro.com https://submit.jotform.com https://www.docdroid.com https://www.docdroid.net https://kaggle-static.storage.googleapis.com https://kaggle-static-staging.storage.googleapis.com https://kkb-dev.jupyter-proxy.kaggle.net https://kkb-staging.jupyter-proxy.kaggle.net https://kkb-production.jupyter-proxy.kaggle.net https://kkb-dev.firebaseapp.com https://kkb-staging.firebaseapp.com https://kkb-production.firebaseapp.com https://kaggle-metastore-test.firebaseapp.com https://kaggle-metastore.firebaseapp.com https://apis.google.com https://content-sheets.googleapis.com/ https://accounts.google.com/ https://storage.googleapis.com https://docs.google.com https://drive.google.com https://calendar.google.com/; base-uri 'none'; report-uri https://csp.withgoogle.com/csp/kaggle/20201130;\", 'X-Content-Type-Options': 'nosniff', 'Referrer-Policy': 'strict-origin-when-cross-origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\OneDrive\\Documenten\\HVA_Assignments\\Projects\\blog_assignment\\blog.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documenten/HVA_Assignments/Projects/blog_assignment/blog.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Use the Kaggle API to download the datasets we're using during the project\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documenten/HVA_Assignments/Projects/blog_assignment/blog.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m api\u001b[39m.\u001b[39;49mdataset_download_file(\u001b[39m\"\u001b[39;49m\u001b[39mluiscorter/netflix-original-films-imdb-scores\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documenten/HVA_Assignments/Projects/blog_assignment/blog.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m file_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mNetflixOriginals.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documenten/HVA_Assignments/Projects/blog_assignment/blog.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m api\u001b[39m.\u001b[39mdataset_download_file(\u001b[39m\"\u001b[39m\u001b[39mariyoomotade/netflix-data-cleaning-analysis-and-visualization\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documenten/HVA_Assignments/Projects/blog_assignment/blog.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m file_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnetflix1.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documenten/HVA_Assignments/Projects/blog_assignment/blog.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m api\u001b[39m.\u001b[39mdataset_download_file(\u001b[39m\"\u001b[39m\u001b[39makpmpr/updated-netflix-stock-price-all-time\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documenten/HVA_Assignments/Projects/blog_assignment/blog.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m file_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnetflix.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:1182\u001b[0m, in \u001b[0;36mKaggleApi.dataset_download_file\u001b[1;34m(self, dataset, file_name, path, force, quiet)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1179\u001b[0m     effective_path \u001b[39m=\u001b[39m path\n\u001b[0;32m   1181\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response(\n\u001b[1;32m-> 1182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets_download_file_with_http_info(\n\u001b[0;32m   1183\u001b[0m         owner_slug\u001b[39m=\u001b[39;49mowner_slug,\n\u001b[0;32m   1184\u001b[0m         dataset_slug\u001b[39m=\u001b[39;49mdataset_slug,\n\u001b[0;32m   1185\u001b[0m         file_name\u001b[39m=\u001b[39;49mfile_name,\n\u001b[0;32m   1186\u001b[0m         _preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[0;32m   1187\u001b[0m url \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mretries\u001b[39m.\u001b[39mhistory[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mredirect_location\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1188\u001b[0m outfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(effective_path, url\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\api\\kaggle_api.py:1579\u001b[0m, in \u001b[0;36mKaggleApi.datasets_download_file_with_http_info\u001b[1;34m(self, owner_slug, dataset_slug, file_name, **kwargs)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[39m# Authentication setting\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m auth_settings \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbasicAuth\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m-> 1579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[0;32m   1580\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m/datasets/download/\u001b[39;49m\u001b[39m{ownerSlug}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{datasetSlug}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{fileName}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m   1581\u001b[0m     path_params,\n\u001b[0;32m   1582\u001b[0m     query_params,\n\u001b[0;32m   1583\u001b[0m     header_params,\n\u001b[0;32m   1584\u001b[0m     body\u001b[39m=\u001b[39;49mbody_params,\n\u001b[0;32m   1585\u001b[0m     post_params\u001b[39m=\u001b[39;49mform_params,\n\u001b[0;32m   1586\u001b[0m     files\u001b[39m=\u001b[39;49mlocal_var_files,\n\u001b[0;32m   1587\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mResult\u001b[39;49m\u001b[39m'\u001b[39;49m,  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m   1588\u001b[0m     auth_settings\u001b[39m=\u001b[39;49mauth_settings,\n\u001b[0;32m   1589\u001b[0m     async_req\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m   1590\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m   1591\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m   1592\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m   1593\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mcollection_formats)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\api_client.py:329\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \n\u001b[0;32m    294\u001b[0m \u001b[39mTo make an async request, set the async_req parameter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[1;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[0;32m    330\u001b[0m                            path_params, query_params, header_params,\n\u001b[0;32m    331\u001b[0m                            body, post_params, files,\n\u001b[0;32m    332\u001b[0m                            response_type, auth_settings,\n\u001b[0;32m    333\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[0;32m    334\u001b[0m                            _preload_content, _request_timeout)\n\u001b[0;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     thread \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[0;32m    337\u001b[0m                                    method, path_params, query_params,\n\u001b[0;32m    338\u001b[0m                                    header_params, body,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m                                    collection_formats,\n\u001b[0;32m    343\u001b[0m                                    _preload_content, _request_timeout))\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\api_client.py:161\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    158\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfiguration\u001b[39m.\u001b[39mhost \u001b[39m+\u001b[39m resource_path\n\u001b[0;32m    160\u001b[0m \u001b[39m# perform request and return response\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m response_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    162\u001b[0m     method, url, query_params\u001b[39m=\u001b[39;49mquery_params, headers\u001b[39m=\u001b[39;49mheader_params,\n\u001b[0;32m    163\u001b[0m     post_params\u001b[39m=\u001b[39;49mpost_params, body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    164\u001b[0m     _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    165\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49m_request_timeout)\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_response \u001b[39m=\u001b[39m response_data\n\u001b[0;32m    169\u001b[0m return_data \u001b[39m=\u001b[39m response_data\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\api_client.py:351\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[39m\"\"\"Makes the HTTP request using RESTClient.\"\"\"\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_client\u001b[39m.\u001b[39;49mGET(url,\n\u001b[0;32m    352\u001b[0m                                 query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[0;32m    353\u001b[0m                                 _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    354\u001b[0m                                 _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[0;32m    355\u001b[0m                                 headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    356\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    357\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mHEAD(url,\n\u001b[0;32m    358\u001b[0m                                  query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[0;32m    359\u001b[0m                                  _preload_content\u001b[39m=\u001b[39m_preload_content,\n\u001b[0;32m    360\u001b[0m                                  _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[0;32m    361\u001b[0m                                  headers\u001b[39m=\u001b[39mheaders)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\rest.py:247\u001b[0m, in \u001b[0;36mRESTClientObject.GET\u001b[1;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGET\u001b[39m(\u001b[39mself\u001b[39m, url, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, query_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _preload_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m         _request_timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 247\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url,\n\u001b[0;32m    248\u001b[0m                         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    249\u001b[0m                         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    250\u001b[0m                         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[0;32m    251\u001b[0m                         query_params\u001b[39m=\u001b[39;49mquery_params)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\rest.py:241\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[1;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    238\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mresponse body: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, r\u001b[39m.\u001b[39mdata)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m299\u001b[39m:\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mraise\u001b[39;00m ApiException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[0;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[1;31mApiException\u001b[0m: (401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Content-Length': '0', 'Date': 'Tue, 27 Sep 2022 07:57:38 GMT', 'Access-Control-Allow-Credentials': 'true', 'Set-Cookie': 'ka_sessionid=398f32e211b231df23a997d72eb1035e; max-age=2626560; path=/, GCLB=CJi2rd6S_oi5kwE; path=/; HttpOnly', 'Turbolinks-Location': 'https://www.kaggle.com/api/v1/datasets/download/luiscorter/netflix-original-films-imdb-scores/NetflixOriginals.csv', 'Strict-Transport-Security': 'max-age=63072000; includeSubDomains; preload', 'Content-Security-Policy': \"object-src 'none'; script-src 'nonce-kqx/Ru4KIml5TH9I82XBVA==' 'report-sample' 'unsafe-inline' 'unsafe-eval' 'strict-dynamic' https: http:; frame-src 'self' https://www.kaggleusercontent.com https://www.youtube.com/embed/ https://polygraph-cool.github.io https://www.google.com/recaptcha/ https://form.jotform.com https://submit.jotform.us https://submit.jotformpro.com https://submit.jotform.com https://www.docdroid.com https://www.docdroid.net https://kaggle-static.storage.googleapis.com https://kaggle-static-staging.storage.googleapis.com https://kkb-dev.jupyter-proxy.kaggle.net https://kkb-staging.jupyter-proxy.kaggle.net https://kkb-production.jupyter-proxy.kaggle.net https://kkb-dev.firebaseapp.com https://kkb-staging.firebaseapp.com https://kkb-production.firebaseapp.com https://kaggle-metastore-test.firebaseapp.com https://kaggle-metastore.firebaseapp.com https://apis.google.com https://content-sheets.googleapis.com/ https://accounts.google.com/ https://storage.googleapis.com https://docs.google.com https://drive.google.com https://calendar.google.com/; base-uri 'none'; report-uri https://csp.withgoogle.com/csp/kaggle/20201130;\", 'X-Content-Type-Options': 'nosniff', 'Referrer-Policy': 'strict-origin-when-cross-origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n"
     ]
    }
   ],
   "source": [
    "# Use the Kaggle API to download the datasets we're using during the project\n",
    "api.dataset_download_file(\"luiscorter/netflix-original-films-imdb-scores\",\n",
    "file_name=\"NetflixOriginals.csv\")\n",
    "\n",
    "api.dataset_download_file(\"ariyoomotade/netflix-data-cleaning-analysis-and-visualization\",\n",
    "file_name=\"netflix1.csv\")\n",
    "\n",
    "api.dataset_download_file(\"akpmpr/updated-netflix-stock-price-all-time\",\n",
    "file_name=\"netflix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with encoded CSV files\n",
    "\n",
    "Before we can proceed with loading the CSV files with Pandas there is one more step we need to take.\n",
    "The CSV files are encoded, and we'll use the chardet library to discover what type of encoding it is.\n",
    "After which we'll use the `encoding parameter` of Pandas to properly load the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetflixOriginals.csv {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n",
      "netflix.csv {'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n",
      "netflix1.csv {'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Import the needed library\n",
    "import chardet\n",
    "\n",
    "# Create a dict with file paths\n",
    "files = {\"NetflixOriginals.csv\": \"./data/NetflixOriginals.csv\", \"netflix.csv\": \"./data/netflix.csv\", \"netflix1.csv\": \"./data/netflix1.csv\"}\n",
    "\n",
    "# Loop through the dict, and print out the names and encoding type \n",
    "for name, file in files.items():\n",
    "    with open(file, 'rb') as rawdata:\n",
    "        result = chardet.detect(rawdata.read(100000))\n",
    "    print(name, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in and Merging the Dataframes\n",
    "\n",
    "Now we can start loading the datasets into Pandas with the `read_csv` function, and we will use the `merge` function to join the dataframes together.\n",
    "\n",
    "- Note: It's only mandatory to use the encoding parameter for the Windows-1252 enconding, because Pandas doesn't have problems with loading in the other encoding formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Date</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enter the Anime</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>58</td>\n",
       "      <td>2.5</td>\n",
       "      <td>English/Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dark Forces</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>81</td>\n",
       "      <td>2.6</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The App</td>\n",
       "      <td>Science fiction/Drama</td>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>79</td>\n",
       "      <td>2.6</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Open House</td>\n",
       "      <td>Horror thriller</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>94</td>\n",
       "      <td>3.2</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kaali Khuhi</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>90</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                  Genre       Date  Runtime  IMDB Score  \\\n",
       "0  Enter the Anime            Documentary 2019-08-05       58         2.5   \n",
       "1      Dark Forces               Thriller 2020-08-21       81         2.6   \n",
       "2          The App  Science fiction/Drama 2019-12-26       79         2.6   \n",
       "3   The Open House        Horror thriller 2018-01-19       94         3.2   \n",
       "4      Kaali Khuhi                Mystery 2020-10-30       90         3.4   \n",
       "\n",
       "           Language  \n",
       "0  English/Japanese  \n",
       "1           Spanish  \n",
       "2           Italian  \n",
       "3           English  \n",
       "4             Hindi  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the first dataset, and decode the csv file with Windows-1252 encoding\n",
    "df = pd.read_csv(\"./data/NetflixOriginals.csv\", encoding=\"Windows-1252\")\n",
    "\n",
    "# Change the column name of Premiere and set the format in DateTime to match the other dataset \n",
    "df.rename(columns = {\"Premiere\": \"Date\"}, inplace=True)\n",
    "df.rename(columns = {\"Title\": \"title\"}, inplace=True)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Dick Johnson Is Dead</td>\n",
       "      <td>Kirsten Johnson</td>\n",
       "      <td>United States</td>\n",
       "      <td>9/25/2021</td>\n",
       "      <td>2020</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>90 min</td>\n",
       "      <td>Documentaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Ganglands</td>\n",
       "      <td>Julien Leclercq</td>\n",
       "      <td>France</td>\n",
       "      <td>9/24/2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Crime TV Shows, International TV Shows, TV Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s6</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Midnight Mass</td>\n",
       "      <td>Mike Flanagan</td>\n",
       "      <td>United States</td>\n",
       "      <td>9/24/2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>TV Dramas, TV Horror, TV Mysteries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s14</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Confessions of an Invisible Girl</td>\n",
       "      <td>Bruno Garotti</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>9/22/2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-PG</td>\n",
       "      <td>91 min</td>\n",
       "      <td>Children &amp; Family Movies, Comedies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s8</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Sankofa</td>\n",
       "      <td>Haile Gerima</td>\n",
       "      <td>United States</td>\n",
       "      <td>9/24/2021</td>\n",
       "      <td>1993</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>125 min</td>\n",
       "      <td>Dramas, Independent Movies, International Movies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id     type                             title         director  \\\n",
       "0      s1    Movie              Dick Johnson Is Dead  Kirsten Johnson   \n",
       "1      s3  TV Show                         Ganglands  Julien Leclercq   \n",
       "2      s6  TV Show                     Midnight Mass    Mike Flanagan   \n",
       "3     s14    Movie  Confessions of an Invisible Girl    Bruno Garotti   \n",
       "4      s8    Movie                           Sankofa     Haile Gerima   \n",
       "\n",
       "         country date_added  release_year rating  duration  \\\n",
       "0  United States  9/25/2021          2020  PG-13    90 min   \n",
       "1         France  9/24/2021          2021  TV-MA  1 Season   \n",
       "2  United States  9/24/2021          2021  TV-MA  1 Season   \n",
       "3         Brazil  9/22/2021          2021  TV-PG    91 min   \n",
       "4  United States  9/24/2021          1993  TV-MA   125 min   \n",
       "\n",
       "                                           listed_in  \n",
       "0                                      Documentaries  \n",
       "1  Crime TV Shows, International TV Shows, TV Act...  \n",
       "2                 TV Dramas, TV Horror, TV Mysteries  \n",
       "3                 Children & Family Movies, Comedies  \n",
       "4   Dramas, Independent Movies, International Movies  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the second dataset\n",
    "df2 = pd.read_csv(\"./data/netflix1.csv\")\n",
    "\n",
    "# Change the column name of title to match the other dataset\n",
    "# df2.rename(columns = {\"title\": \"Title\"}, inplace=True)\n",
    "\n",
    "# Show the first 5 rows\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-05-23</td>\n",
       "      <td>1.242857</td>\n",
       "      <td>1.145714</td>\n",
       "      <td>1.156429</td>\n",
       "      <td>1.196429</td>\n",
       "      <td>104790000.0</td>\n",
       "      <td>1.196429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-05-24</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>1.197143</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>11104800.0</td>\n",
       "      <td>1.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-05-28</td>\n",
       "      <td>1.232143</td>\n",
       "      <td>1.157143</td>\n",
       "      <td>1.213571</td>\n",
       "      <td>1.157143</td>\n",
       "      <td>6609400.0</td>\n",
       "      <td>1.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-05-29</td>\n",
       "      <td>1.164286</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.164286</td>\n",
       "      <td>1.103571</td>\n",
       "      <td>6757800.0</td>\n",
       "      <td>1.103571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-05-30</td>\n",
       "      <td>1.107857</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.107857</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>10154200.0</td>\n",
       "      <td>1.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      High       Low      Open     Close       Volume  Adj Close\n",
       "0 2002-05-23  1.242857  1.145714  1.156429  1.196429  104790000.0   1.196429\n",
       "1 2002-05-24  1.225000  1.197143  1.214286  1.210000   11104800.0   1.210000\n",
       "2 2002-05-28  1.232143  1.157143  1.213571  1.157143    6609400.0   1.157143\n",
       "3 2002-05-29  1.164286  1.085714  1.164286  1.103571    6757800.0   1.103571\n",
       "4 2002-05-30  1.107857  1.071429  1.107857  1.071429   10154200.0   1.071429"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the third dataset\n",
    "df3 = pd.read_csv(\"./data/netflix.csv\")\n",
    "\n",
    "df3[\"Date\"] = pd.to_datetime(df3[\"Date\"])\n",
    "\n",
    "# Show the first 5 rows\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When merging Dataframes there are multiple options on how to do it:\n",
    "\n",
    "- Inner: keep all rows from both Dataframes that match (Which can result in dataloss in our case)\n",
    "\n",
    "- Outer: keep all rows from both Dataframes (Which results in alot of rows with NaN values in our case)\n",
    "\n",
    "- Left: Include all rows from Dataframe X and only those from y that match (X= target Dataframe, y= Dataframe to merge in)\n",
    "\n",
    "- Right: Inculde all from Dataframe y and only those from X that match\n",
    "\n",
    "We've made the the choice to use a Left Join, because Dataframe X is alot shorter then y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Date</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Language</th>\n",
       "      <th>rating</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enter the Anime</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>58</td>\n",
       "      <td>2.5</td>\n",
       "      <td>English/Japanese</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>313.420013</td>\n",
       "      <td>304.679993</td>\n",
       "      <td>310.959991</td>\n",
       "      <td>307.630005</td>\n",
       "      <td>8692500.0</td>\n",
       "      <td>307.630005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dark Forces</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>81</td>\n",
       "      <td>2.6</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>498.130005</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>496.459991</td>\n",
       "      <td>492.309998</td>\n",
       "      <td>3921300.0</td>\n",
       "      <td>492.309998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The App</td>\n",
       "      <td>Science fiction/Drama</td>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>79</td>\n",
       "      <td>2.6</td>\n",
       "      <td>Italian</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>336.459991</td>\n",
       "      <td>332.010010</td>\n",
       "      <td>334.600006</td>\n",
       "      <td>332.630005</td>\n",
       "      <td>3589900.0</td>\n",
       "      <td>332.630005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Open House</td>\n",
       "      <td>Horror thriller</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>94</td>\n",
       "      <td>3.2</td>\n",
       "      <td>English</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>223.490005</td>\n",
       "      <td>218.500000</td>\n",
       "      <td>222.750000</td>\n",
       "      <td>220.460007</td>\n",
       "      <td>10548600.0</td>\n",
       "      <td>220.460007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kaali Khuhi</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>90</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Hindi</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>505.880005</td>\n",
       "      <td>472.209991</td>\n",
       "      <td>502.010010</td>\n",
       "      <td>475.739990</td>\n",
       "      <td>7807900.0</td>\n",
       "      <td>475.739990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                  Genre       Date  Runtime  IMDB Score  \\\n",
       "0  Enter the Anime            Documentary 2019-08-05       58         2.5   \n",
       "1      Dark Forces               Thriller 2020-08-21       81         2.6   \n",
       "2          The App  Science fiction/Drama 2019-12-26       79         2.6   \n",
       "3   The Open House        Horror thriller 2018-01-19       94         3.2   \n",
       "4      Kaali Khuhi                Mystery 2020-10-30       90         3.4   \n",
       "\n",
       "           Language rating        High         Low        Open       Close  \\\n",
       "0  English/Japanese  TV-MA  313.420013  304.679993  310.959991  307.630005   \n",
       "1           Spanish  TV-MA  498.130005  490.000000  496.459991  492.309998   \n",
       "2           Italian  TV-MA  336.459991  332.010010  334.600006  332.630005   \n",
       "3           English  TV-MA  223.490005  218.500000  222.750000  220.460007   \n",
       "4             Hindi  TV-14  505.880005  472.209991  502.010010  475.739990   \n",
       "\n",
       "       Volume   Adj Close  \n",
       "0   8692500.0  307.630005  \n",
       "1   3921300.0  492.309998  \n",
       "2   3589900.0  332.630005  \n",
       "3  10548600.0  220.460007  \n",
       "4   7807900.0  475.739990  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe by merging df, df2, df3 on shared column names and using the Left Join\n",
    "netflix_df = df.merge(df2[[\"title\", \"rating\"]], on=\"title\", how=\"left\") \\\n",
    "        .merge(df3, on=\"Date\", how=\"left\") \n",
    "netflix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 584 entries, 0 to 583\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   title       584 non-null    object        \n",
      " 1   Genre       584 non-null    object        \n",
      " 2   Date        584 non-null    datetime64[ns]\n",
      " 3   Runtime     584 non-null    int64         \n",
      " 4   IMDB Score  584 non-null    float64       \n",
      " 5   Language    584 non-null    object        \n",
      " 6   rating      504 non-null    object        \n",
      " 7   High        548 non-null    float64       \n",
      " 8   Low         548 non-null    float64       \n",
      " 9   Open        548 non-null    float64       \n",
      " 10  Close       548 non-null    float64       \n",
      " 11  Volume      548 non-null    float64       \n",
      " 12  Adj Close   548 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(7), int64(1), object(4)\n",
      "memory usage: 63.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows and datatypes\n",
    "netflix_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "Genre          0\n",
       "Date           0\n",
       "Runtime        0\n",
       "IMDB Score     0\n",
       "Language       0\n",
       "rating        80\n",
       "High          36\n",
       "Low           36\n",
       "Open          36\n",
       "Close         36\n",
       "Volume        36\n",
       "Adj Close     36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are NaN values\n",
    "netflix_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rating column and all the columns with Stock data have NaN values. We now have to decide what we want to do with the those rows, there are two main options for this: `pd.fillna` or `pd.dropna`. \n",
    "\n",
    "##### Rating column\n",
    "For the Rating column we have decided to fill all NaN values with U for unknown, because 80 rows would be alot of data to discard.\n",
    "\n",
    "##### Stock data columns\n",
    "For the Stock data columns we've tried to apply the `pd.interpolate` function, but it wasn't succesfull . We've decided to drop the rows in question, because dropping 36 rows could be better justified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all values in the Dataframe by date, to order the data first\n",
    "netflix_df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "# Fill the NaN values of the rating column with U \n",
    "netflix_df[\"rating\"].fillna(\"U\", inplace=True)\n",
    "\n",
    "# Drop all rows with NaN values\n",
    "netflix_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 548 entries, 519 to 369\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   title       548 non-null    object        \n",
      " 1   Genre       548 non-null    object        \n",
      " 2   Date        548 non-null    datetime64[ns]\n",
      " 3   Runtime     548 non-null    int64         \n",
      " 4   IMDB Score  548 non-null    float64       \n",
      " 5   Language    548 non-null    object        \n",
      " 6   rating      548 non-null    object        \n",
      " 7   High        548 non-null    float64       \n",
      " 8   Low         548 non-null    float64       \n",
      " 9   Open        548 non-null    float64       \n",
      " 10  Close       548 non-null    float64       \n",
      " 11  Volume      548 non-null    float64       \n",
      " 12  Adj Close   548 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(7), int64(1), object(4)\n",
      "memory usage: 59.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows and datatypes again\n",
    "netflix_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         0\n",
       "Genre         0\n",
       "Date          0\n",
       "Runtime       0\n",
       "IMDB Score    0\n",
       "Language      0\n",
       "rating        0\n",
       "High          0\n",
       "Low           0\n",
       "Open          0\n",
       "Close         0\n",
       "Volume        0\n",
       "Adj Close     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of NaN values again\n",
    "netflix_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Date</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Language</th>\n",
       "      <th>rating</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>The Other One: The Long Strange Trip of Bob Weir</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2015-05-22</td>\n",
       "      <td>83</td>\n",
       "      <td>7.3</td>\n",
       "      <td>English</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>89.407143</td>\n",
       "      <td>88.692856</td>\n",
       "      <td>89.178574</td>\n",
       "      <td>88.838570</td>\n",
       "      <td>7466200.0</td>\n",
       "      <td>88.838570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Hot Girls Wanted</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2015-05-29</td>\n",
       "      <td>84</td>\n",
       "      <td>6.1</td>\n",
       "      <td>English</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>90.205711</td>\n",
       "      <td>88.955711</td>\n",
       "      <td>89.472855</td>\n",
       "      <td>89.151428</td>\n",
       "      <td>12712000.0</td>\n",
       "      <td>89.151428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>What Happened, Miss Simone?</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2015-06-26</td>\n",
       "      <td>84</td>\n",
       "      <td>7.6</td>\n",
       "      <td>English</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>95.314285</td>\n",
       "      <td>93.088570</td>\n",
       "      <td>95.300003</td>\n",
       "      <td>93.088570</td>\n",
       "      <td>30314900.0</td>\n",
       "      <td>93.088570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Tig</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>80</td>\n",
       "      <td>7.4</td>\n",
       "      <td>English</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>117.879997</td>\n",
       "      <td>114.239998</td>\n",
       "      <td>117.339996</td>\n",
       "      <td>114.769997</td>\n",
       "      <td>25136900.0</td>\n",
       "      <td>114.769997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>Keith Richards: Under the Influence</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>81</td>\n",
       "      <td>7.1</td>\n",
       "      <td>English</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>104.360001</td>\n",
       "      <td>100.709999</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.620003</td>\n",
       "      <td>21715000.0</td>\n",
       "      <td>102.620003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title        Genre       Date  \\\n",
       "519  The Other One: The Long Strange Trip of Bob Weir  Documentary 2015-05-22   \n",
       "228                                  Hot Girls Wanted  Documentary 2015-05-29   \n",
       "550                       What Happened, Miss Simone?  Documentary 2015-06-26   \n",
       "531                                               Tig  Documentary 2015-07-17   \n",
       "465               Keith Richards: Under the Influence  Documentary 2015-09-18   \n",
       "\n",
       "     Runtime  IMDB Score Language rating        High         Low        Open  \\\n",
       "519       83         7.3  English  TV-14   89.407143   88.692856   89.178574   \n",
       "228       84         6.1  English  TV-MA   90.205711   88.955711   89.472855   \n",
       "550       84         7.6  English  TV-14   95.314285   93.088570   95.300003   \n",
       "531       80         7.4  English  TV-14  117.879997  114.239998  117.339996   \n",
       "465       81         7.1  English  TV-14  104.360001  100.709999  102.000000   \n",
       "\n",
       "          Close      Volume   Adj Close  \n",
       "519   88.838570   7466200.0   88.838570  \n",
       "228   89.151428  12712000.0   89.151428  \n",
       "550   93.088570  30314900.0   93.088570  \n",
       "531  114.769997  25136900.0  114.769997  \n",
       "465  102.620003  21715000.0  102.620003  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 5 rows\n",
    "netflix_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
